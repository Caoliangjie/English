\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{ctex}
\usepackage{CJK}
\usepackage{times}
\usepackage{fontspec}
\usepackage{graphicx}
\usepackage{cvpr}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cite}
\usepackage{multirow}
\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\usepackage{geometry}
\usepackage{latexsym}
\setmainfont{Times New Roman}
\usepackage{setspace}
\setlength{\parindent}{2em}
\cvprfinalcopy
\def\cvprPaperID{****}
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\begin{document}
\title{\textbf{Learning Deep Representations of Fine-Grained Visual Descriptions}}
\author{Liangjie Cao\\31 May 2018}
\maketitle
\section{Flowers zero-shot recognition and retrieval}
To demonstrate that their results generalize beyond the case of bird images, they report the same set of experiments on the Flowers dataset. All neural text model architectures are the same as they used for CUB, and they used the same hyperparameters from crossvalidation on CUB. Table~\ref{Table1} summarizes their results. From the Table, Char CNN-RNN achieves competitive results to wordlevel models both for DA-SJE and DS-SJE. The wordlevel models achieve the best result, significantly better than both the shallow embeddings and character-level models. Among different models, Word LSTM is the winner for DASJE both in classification and retrieval. On the other hand, Word CNN-RNN is the winner for DS-SJE for the same. As in the case for CUB, DS-SJE achieves strong retrieval performance, and DA-SJE often fails in comparison.
\begin{figure}[!htb]
 \centering
 \includegraphics[width=0.5\textwidth]{compare.png}\\
 \caption{\textbf{Zero-shot retrieval given a single query sentence. Each row corresponds to a different text encoder.}}\label{Figure1}
 \end{figure}
\section{Qualitative results}
Figure~\ref{Figure1} shows several example zero-shot retrieval results using a single text description. Both the text queries and images are real data points drawn from the test set. The authors observe that having trained on our dataset of visual descriptions, our proposed method returns results that accurately reflect the text, even when using only a single caption. Quantitatively, BoW achieves 14.6\% AP@50 with a single query compared to 18.0\% with word-LSTM and 20.7\% with Word-CNN-RNN. Note that although almost all retrieved images match the text query well, the actual class of that image can still be incorrect. This is why the average precision may seem low compared to the generally good qualitative results. The performance appears to degrade gracefully; our model at least returns visually-consistent results if not of the correct class. And they show a t-SNE embedding of test-set description embeddings in Figure~\ref{Figure2}, successfully clustering according to visual similarities (i.e. color, shape). Additional examples from test images and queries are included in the supplementary material.
 \begin{figure}[htbp]
 \centering
 \includegraphics[width=0.5\textwidth]{zoom.png}\\
 \caption{\textbf{t-SNE embedding of test class description embeddings from Oxford-102 (left) and CUB (right), marked with corresponding images. Best viewed with zoom.}}\label{Figure2}
\end{figure}
   \begin{table}[htbp]
  \centering
 \begin{tabular}{|p{2cm}|p{1cm}|p{1cm}|}
    \hline
     Approach & CUB & Flowers  \\
    \hline
     CSHAPH~\cite{name18} & 17.5 & -\\
    \hline
     AHLE~\cite{name1} & 27.3 & - \\
    \hline
     TMV-HLP~\cite{name14} & 47.9 & -\\
    \hline
     SJE~\cite{name2} & 50.1 & -\\
    \hline
     DA-SJE (ours) & 54.3 & 62.3 \\
    \hline
     DS-SJE (ours) & 56.8 & 65.6 \\
    \hline
  \end{tabular}
  \caption{\textbf{Summary of zero-shot {\%} classi?cation accuracies}}\label{Table2}
  \end{table}
   \begin{table}[!htbp]
  \centering
 \begin{tabular}{|p{2cm}|p{1cm}|p{1cm}|p{1cm}|p{0.8cm}|}
    \hline
     & \multicolumn{2}{c|}{Top-1 Acc (\%)} & \multicolumn{2}{c|}{AP@50 (\%)}\\
    \hline
     \textbf{Embedding} & DA-SJE & DS-SJE & DA-SJE & DS-SJE\\
    \hline
     WORD2VEC & 54.6 & 54.2 & 16.3 & 52.1 \\
    \hline
     BAG-OF-WORDS & 56.7 & 57.7 & 28.2 & 57.3\\
    \hline
     CHAR CNN & 51.1 & 47.3 & 8.3 & 46.1\\
    \hline
     CHAR LSTM & 29.1 & 25.8 & 19.3 & 27.0\\
    \hline
     CHAR CNN-RNN & 61.7 & 63.7 & 13.6 & 57.3\\
    \hline
     WORD CNN & 60.2 & 60.7 & 8.7 & 56.3\\
    \hline
     WORD LSTM & 62.3 & 64.5 & 45.9 & 52.3 \\
    \hline
     WORD CNN-RNN & 60.9 & 65.6 & 7.6 & 59.6\\
    \hline
  \end{tabular}
  \caption{\textbf{Zero-shot {\%} recognition accuracy and retrieval average precision on Flowers}}\label{Table1}
  \end{table}
\par
\section{Comparison to the state-of-the-art}
In this section they compare to the previously published results on CUB, including results that use the same zeroshot split. CSHAPH~\cite{name18} uses 4K-dim features from the Oxford VGG net~\cite{name40} and also attributes to learn a hypergraph on the attribute space. AHLE~\cite{name1} uses Fisher vector image features and attribute embeddings to learn a bilinear compatibility function between these embeddings. TMVHLP~\cite{name14} builds a hypergraph on a multiview embedding space learned via CCA which uses deep image features and attributes. In SJE~\cite{name2} as in AHLE~\cite{name1} a compatibility function is learned, in this case between 1K-dim GoogleNet~\cite{name44} features and various other embeddings including attributes. Overall, the results in Table~\ref{Table2} demonstrate that state-ofthe-art zero-shot prediction performace can be achieved directly from text descriptions. This does not require access to any form of test label embeddings. Although attributes are richer and more compact than text descriptions, attributes alone form a very small training set. 
\section{Discussion}
Their visual descriptions data also improved the zero shot accuracy using BoW and word2vec encoders. While these win in the smaller data regime, higher capacity encoders dominate when enough data is available. Thus our contributions (data, objective and text encoders) improve performance at multiple operating points of training text size.
\bibliographystyle{abbrv}
\bibliography{yinyong1}
\end{document}

