\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{ctex}
\usepackage{CJK}
\usepackage{times}
\usepackage{graphicx}
\usepackage{cvpr}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cite}
\usepackage{multirow}
\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\usepackage{geometry}
\setmainfont{Times New Roman}
\usepackage{setspace}
\setlength{\parindent}{2em}
\cvprfinalcopy
\def\cvprPaperID{****}
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\begin{document}
\title{\textbf{Learning Deep Representations of Fine-Grained Visual Descriptions}}
\author{Liangjie Cao\\25 May 2018}
\maketitle
\par
\section{Introduction}
State-of-the-art methods for zero-shot visual recognition formulate learning as a joint embedding problem of images and side information. Their proposed models train end-to-end to align with the fine-grained and category-specific content of images. Actually natural language provides a flexible and compact way of encoding only the salient visual aspects for distinguishing categories. By training on raw text, our model can do inference on raw text as well, providing humans a familiar mode both for annotation and retrieval. A key challenge in image understanding is to correctly relate natural language concepts to the visual content of images. In recent years there has been significant progress in learning visual-semantic embeddings, \emph{e.g.} for zero-shot learning~\cite{name36,name38,name24,name33,name12,name41,name2}, and automatically generating image captions for general web images.~\cite{name23,name35,name45,name20,name8}\par
 The authors contributions in this work are as follows. First, they collected two datasets of ne-grained visual descriptions: one for the Caltech-UCSD birds dataset, and another for the Oxford-102 flowers dataset~\cite{name32}. Both data and code will be made available. Second, they propose a novel extension of structured joint embedding~\cite{name2}, and show that it can be used for end-to-end training of deep neural language models. It also dramatically improves zero-shot retrieval performance for all models. Third, they evaluate several variants of word- and character-based neural language models, including our novel hybrids of convolutional and recurrent networks for text modeling. They demonstrate signicant improvements over the state-of-the-art on CUB and Flowers datasets in both zero-shot recognition and retrieval.
\section{Related work}
 In the past few years, advances in deep convolutional networks~\cite{name22,name9,name44} have driven rapid progress in general-purpose visual recognition on large-scale benchmarks such as ImageNet.~\cite{name6} The learned features of these networks have proven transferable to many other problems~\cite{name34}. However, a remaining challenge is finegrained image classifcation~\cite{name46,name7,name10,name51}, \emph{i.e.} classifying objects of many visually similar classes. The difficulty is increased by the lack of extensive labeled images, which for fine-grained data sets may even require annotation by human experts. They demonstrate that with suf?cient training data, text-based label embeddings can outperform the previous attributes-based state-of-the art for zero-shot recognition on CUB (at both word and character level). It is also possible to build an asymmetric model in the opposite direction, \emph{i.e.} only train ft in order to perform zero-shot image retrieval, although we are not aware of previous works doing this. From a practical perspective it is clearly better to have a single model that does both tasks well. Thus in the experiments they compare DS-SJE with DA-SJE (training only fv) for zero-shot classi?cation.
  \begin{table}[!htbp]
  \centering
 \begin{tabular}{|p{2cm}|p{2cm}|}
    \hline
    1 & Translation language\\
    \hline
     2 & Control robot\\
    \hline
         3 & Image analysis\\
    \hline
        4 & Image analysis\\
    \hline
  \end{tabular}
  \caption{\textbf{What can lstm - based system do}} \label{Table1}
  \end{table}
\par
\section{Deep Structured Joint Embedding}
As in previous multimodal structured learning methods, the authors learn a compatibility function of images and text. However, instead of using a bilinear compatibility function we use the inner product of features generated by deep neural encoders. An instantiation of our model using a word-level LSTM(Table~\ref{Table1}) is illustrated in Figure~\ref{Figure1}. Since their text encoder models are all differentiable, their backpropagate (sub)-gradients through all text network parameters for end-to-end training. For the image encoder, they keep the network weights ?xed to the original GoogLeNet. I will continue learning the following days.
 \begin{figure}[htbp]
 \centering
 \includegraphics[width=0.5\textwidth]{Fmod.png}\\
 \caption{\textbf{RON object detection overview}}\label{Figure1}
\end{figure}
\bibliographystyle{plain}
\bibliography{yinyong1}
\end{document}

